{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3267e7b9",
   "metadata": {
    "id": "3267e7b9"
   },
   "source": [
    "# Course AI Homework 5\n",
    "In Homework 5, we will train our own 'CBOW' Word2Vec embedding from WikiText2 dataset. (small dataset)\n",
    "- Change Runtime option above to GPU if you could. (max 12 hours for one user)\n",
    "- Save and submit the outputs of this notebook and model and vocab file you trained.\n",
    "- Not allowed to have other python file or import pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c26099",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'py13 (Python 3.13.5)'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m필요한 패키지를 사용하여 <a href='command:jupyter.createPythonEnvAndSelectController'>Python 환경 만들기</a>\n",
      "\u001b[1;31m또는 다음 명령을 사용하여 'ipykernel'을(를) 설치합니다. 'conda install -n py13 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "! pip uninstall torch -y\n",
    "! pip install torch==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fEUOIf5b55uz",
   "metadata": {
    "id": "fEUOIf5b55uz"
   },
   "outputs": [],
   "source": [
    "# YOU should run this command if you will train the model in COLAB environment\n",
    "! pip install datasets transformers torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83999a8f",
   "metadata": {
    "id": "83999a8f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import WikiText2 # WikiText103\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c48ed1",
   "metadata": {
    "id": "d1c48ed1"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_seed_numb = 0\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed(torch_seed_numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020510ad",
   "metadata": {
    "id": "020510ad"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YYgqjYSdBvDW",
   "metadata": {
    "id": "YYgqjYSdBvDW"
   },
   "outputs": [],
   "source": [
    "# If you use Google Colab environment, mount you google drive here to save model and vocab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "root_dir = '/content/drive/MyDrive/course_ai_hw5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BNYyA8HX1Uz-",
   "metadata": {
    "id": "BNYyA8HX1Uz-"
   },
   "source": [
    "### Constant Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f991d",
   "metadata": {
    "id": "7f2f991d"
   },
   "outputs": [],
   "source": [
    "# You could change parameters if you want.\n",
    "\n",
    "train_batch_size =  96\n",
    "val_batch_size = 96\n",
    "shuffle =  True\n",
    "\n",
    "optimizer =  'Adam'\n",
    "learning_rate =  0.025\n",
    "epochs = 50\n",
    "\n",
    "result_dir = 'weights/'\n",
    "\n",
    "# Parameters about CBOW model architecture and Vocab.\n",
    "CBOW_N_WORDS = 4\n",
    "\n",
    "MIN_WORD_FREQUENCY = 50\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "\n",
    "EMBED_DIMENSION = 300\n",
    "EMBED_MAX_NORM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3FgsAeWUDvzI",
   "metadata": {
    "id": "3FgsAeWUDvzI"
   },
   "outputs": [],
   "source": [
    "result_dir = os.path.join(root_dir, result_dir)\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xQacYYHCovzA",
   "metadata": {
    "id": "xQacYYHCovzA"
   },
   "source": [
    "## Prepare dataset and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rCx3DsJr5RxX",
   "metadata": {
    "id": "rCx3DsJr5RxX"
   },
   "outputs": [],
   "source": [
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "train_dataset = datasets[\"train\"]\n",
    "val_dataset = datasets['validation']\n",
    "test_dataset = datasets['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j499GQWrxOma",
   "metadata": {
    "id": "j499GQWrxOma"
   },
   "outputs": [],
   "source": [
    "# Let's print one example\n",
    "train_dataset['text'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulQoM_SixZBT",
   "metadata": {
    "id": "ulQoM_SixZBT"
   },
   "source": [
    "As you can see, we need to clean up the sentences, lowercase them, tokenize them, and change each word into an index (one-hot vector). Before going through the whole process, we need to create a vocab set using the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u63stoTf7oRn",
   "metadata": {
    "id": "u63stoTf7oRn"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "\n",
    "# TODO 1): make vocabulary\n",
    "# Hint) use function: build_vocab_from_iterator, use train_dataset set special tokens.. etc\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_dataset['text']),\n",
    "    min_freq=MIN_WORD_FREQUENCY,\n",
    "    specials=[\"<unk>\", \"<pad>\"]\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i8z22T68aT-m",
   "metadata": {
    "id": "i8z22T68aT-m"
   },
   "source": [
    "We need a collate function to make dataset into CBOW trainning format. The collate function should iterate over (sliding) batch data and make train/test dataset.And each component of data should be composed of CBOW_N_WORD words in each left and right side as input and target output as word in center.  \n",
    "Make the collate function return CBOW dataset in tensor type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CrDMIbXVHcbk",
   "metadata": {
    "id": "CrDMIbXVHcbk"
   },
   "outputs": [],
   "source": [
    "# Here is a lambda function to tokenize sentence and change words to vocab indexes.\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yl65bTOrEuSU",
   "metadata": {
    "id": "yl65bTOrEuSU"
   },
   "source": [
    "![cbow](https://user-images.githubusercontent.com/74028313/204695601-51d44a38-4bd3-4a69-8891-2854aa57c034.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745eedd1",
   "metadata": {
    "id": "745eedd1"
   },
   "outputs": [],
   "source": [
    "def collate(batch, text_pipeline):\n",
    "\n",
    "    batch_input, batch_output = [], []\n",
    "\n",
    "    # TODO 2): make collate function\n",
    "    for text in batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "        \n",
    "        if len(text_tokens_ids) < CBOW_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "        \n",
    "        for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):\n",
    "            context = (\n",
    "                text_tokens_ids[idx : idx + CBOW_N_WORDS] +\n",
    "                text_tokens_ids[idx + CBOW_N_WORDS + 1 : idx + CBOW_N_WORDS * 2 + 1]\n",
    "            )\n",
    "            target = text_tokens_ids[idx + CBOW_N_WORDS]\n",
    "            \n",
    "            batch_input.append(context)\n",
    "            batch_output.append(target)\n",
    "    \n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "\n",
    "    return batch_input, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c062f3",
   "metadata": {
    "id": "b8c062f3"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset['text'],\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=shuffle,\n",
    "    collate_fn=partial(collate, text_pipeline=text_pipeline),\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset['text'],\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=shuffle,\n",
    "    collate_fn=partial(collate, text_pipeline=text_pipeline),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95_n2reEdl8-",
   "metadata": {
    "id": "95_n2reEdl8-"
   },
   "source": [
    "## Make CBOW Model\n",
    "![image](https://user-images.githubusercontent.com/74028313/204701161-cd9df4bf-78b8-4b4d-b8b7-ed4a3b5c3922.png)\n",
    "\n",
    "CBOW Models' main concept is to predict center-target word using context words. As you see in above simple architecture, input 2XCBOW_N_WORDS length words are projected to Projection layer. In order to convert each word to embedding, it needs look-up table and we will use torch's Embedding function to convert it. After combining embeddings of context, it use shallow linear neural network to predict target word and compare result with center word's index using cross-entropy loss. Finally, the embedding layer (lookup table) of the trained model itself serves as an embedding representing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066a121",
   "metadata": {
    "id": "5066a121"
   },
   "outputs": [],
   "source": [
    "class CBOW_Model(nn.Module):\n",
    "    def __init__(self, vocab_size: int, EMBED_DIMENSION, EMBED_MAX_NORM):\n",
    "        super(CBOW_Model, self).__init__()\n",
    "        # TODO 3-1): make CBOW model using nn.Embedding and nn.Linear function\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "            max_norm=EMBED_MAX_NORM\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, _inputs):\n",
    "        # TODO 3-2): make forward function\n",
    "        x = self.embeddings(_inputs)\n",
    "        \n",
    "        x = x.mean(axis=1)\n",
    "        \n",
    "        _outputs = self.linear(x)\n",
    "\n",
    "        return _outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37edd6",
   "metadata": {
    "id": "3e37edd6"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Let's make _train_epoch and _validate_epoch functions to train the CBOW model.  \n",
    "- model.train() and model.eval() change torch mode in some parts (Dropout, BatchNorm..  etc) of the model to behave differently during inference time.\n",
    "- There is lr_scheduler option which changes learning rate according to epoch level. Try the option if you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gN4OgGATqry3",
   "metadata": {
    "id": "gN4OgGATqry3"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.get_stoi())\n",
    "\n",
    "model = CBOW_Model(vocab_size=vocab_size, EMBED_DIMENSION = EMBED_DIMENSION, EMBED_MAX_NORM = EMBED_MAX_NORM)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda8d29",
   "metadata": {
    "id": "0cda8d29"
   },
   "outputs": [],
   "source": [
    "class Train_CBOW:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        epochs,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        device,\n",
    "        model_dir,\n",
    "        lr_scheduler = None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.model_dir = model_dir\n",
    "\n",
    "        self.loss = {\"train\": [], \"val\": []}\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self._train_epoch()\n",
    "            self._validate_epoch()\n",
    "            print(\n",
    "                \"Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}\".format(\n",
    "                    epoch + 1,\n",
    "                    self.epochs,\n",
    "                    self.loss[\"train\"][-1],\n",
    "                    self.loss[\"val\"][-1],\n",
    "                )\n",
    "            )\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "\n",
    "    def _train_epoch(self):\n",
    "        self.model.train() # set model as train\n",
    "        loss_list = []\n",
    "        # TODO 4-1):\n",
    "        for batch_input, batch_output in self.train_dataloader:\n",
    "            batch_input = batch_input.to(self.device)\n",
    "            batch_output = batch_output.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            output = self.model(batch_input)\n",
    "            \n",
    "            loss = self.loss_function(output, batch_output)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "        # end of TODO\n",
    "        epoch_loss = np.mean(loss_list)\n",
    "        self.loss[\"train\"].append(epoch_loss)\n",
    "\n",
    "    def _validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        loss_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # TODO 4-2):\n",
    "            for batch_input, batch_output in self.val_dataloader:\n",
    "                batch_input = batch_input.to(self.device)\n",
    "                batch_output = batch_output.to(self.device)\n",
    "                \n",
    "                output = self.model(batch_input)\n",
    "                \n",
    "                loss = self.loss_function(output, batch_output)\n",
    "                \n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "            # end of TODO\n",
    "        epoch_loss = np.mean(loss_list)\n",
    "        self.loss[\"val\"].append(epoch_loss)\n",
    "\n",
    "\n",
    "    def save_model(self):\n",
    "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
    "        torch.save(self.model, model_path)\n",
    "\n",
    "    def save_loss(self):\n",
    "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
    "        with open(loss_path, \"w\") as fp:\n",
    "            json.dump(self.loss, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcx2Ms537jwR",
   "metadata": {
    "id": "fcx2Ms537jwR"
   },
   "outputs": [],
   "source": [
    "# Option: you could add and change lr_sceduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36edc874",
   "metadata": {
    "id": "36edc874"
   },
   "outputs": [],
   "source": [
    "trainer = Train_CBOW(\n",
    "    model=model,\n",
    "    epochs=epochs,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=None,\n",
    "    device=device,\n",
    "    model_dir=result_dir,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840866c3",
   "metadata": {
    "id": "840866c3"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.save_model()\n",
    "trainer.save_loss()\n",
    "\n",
    "vocab_path = os.path.join(result_dir, \"vocab.pt\")\n",
    "torch.save(vocab, vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373acbe",
   "metadata": {
    "id": "9373acbe"
   },
   "source": [
    "### Result\n",
    "Let's inference trained word embedding and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d19035",
   "metadata": {
    "id": "a6d19035"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fNszs2GvMbbz",
   "metadata": {
    "id": "fNszs2GvMbbz"
   },
   "outputs": [],
   "source": [
    "result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c1ad2",
   "metadata": {
    "id": "ba4c1ad2"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# reload saved model and vocab\n",
    "model = torch.load(os.path.join(result_dir,\"model.pt\"), map_location=device)\n",
    "vocab = torch.load(os.path.join(result_dir,\"vocab.pt\"))\n",
    "\n",
    "# embedding is model's first layer\n",
    "embeddings = list(model.parameters())[0]\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "\n",
    "# normalization\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
    "norms = np.reshape(norms, (len(norms), 1))\n",
    "embeddings_norm = embeddings / norms\n",
    "embeddings_norm.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arTAGVNbuxvP",
   "metadata": {
    "id": "arTAGVNbuxvP"
   },
   "source": [
    "### Make t-SNE graph of trained embedding and color numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2d8a7",
   "metadata": {
    "id": "e4e2d8a7"
   },
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(embeddings_norm)\n",
    "fig = go.Figure()\n",
    "# TODO 5-1) : make 2-d t-SNE graph of all vocabs and color only for numeric values(others, just color black)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=30)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings_norm)\n",
    "\n",
    "words = vocab.get_itos()\n",
    "\n",
    "numeric_words = [word for word in words if word.isdigit()]\n",
    "is_numeric = [word.isdigit() for word in words]\n",
    "\n",
    "colors = []\n",
    "for word in words:\n",
    "    if word.isdigit():\n",
    "        colors.append(int(word) if int(word) < 10 else 9)\n",
    "    else:\n",
    "        colors.append(-1)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=embeddings_tsne[:, 0],\n",
    "    y=embeddings_tsne[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=colors,\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        cmin=0,\n",
    "        cmax=9,\n",
    "    ),\n",
    "    text=words,\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE Visualization of Word Embeddings (Numeric words colored)\",\n",
    "    xaxis_title=\"t-SNE Component 1\",\n",
    "    yaxis_title=\"t-SNE Component 2\",\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(os.path.join(result_dir, \"image.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44537c",
   "metadata": {
    "id": "ce44537c"
   },
   "source": [
    "### Find top N similar words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa05e68",
   "metadata": {
    "id": "4fa05e68"
   },
   "outputs": [],
   "source": [
    "def find_top_similar(word: str, vocab, embeddings_norm, topN: int = 10):\n",
    "    # TODO 5-2) : make function returning top n similiar words and similarity scores\n",
    "    topN_dict = {}\n",
    "    \n",
    "    word_idx = vocab[word]\n",
    "    \n",
    "    word_embedding = embeddings_norm[word_idx]\n",
    "    \n",
    "    similarities = np.dot(embeddings_norm, word_embedding)\n",
    "    \n",
    "    top_indices = np.argsort(similarities)[::-1][:topN+1]\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        similar_word = vocab.lookup_token(idx)\n",
    "        if similar_word != word:\n",
    "            topN_dict[similar_word] = float(similarities[idx])\n",
    "            if len(topN_dict) == topN:\n",
    "                break\n",
    "\n",
    "    return topN_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74887441",
   "metadata": {
    "id": "74887441"
   },
   "outputs": [],
   "source": [
    "for word, sim in find_top_similar(\"english\", vocab, embeddings_norm).items():\n",
    "    print(\"{}: {:.3f}\".format(word, sim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h9WjlWvx0638",
   "metadata": {
    "id": "h9WjlWvx0638"
   },
   "source": [
    "### Result Report\n",
    "\n",
    "Save the colab result and submit it with your trained model file, vocab file, and t-SNE result image in the .zip format. Check one more time your submitted notebook file has result.\n",
    "\n",
    "You can change the CBOW model parameters Training parameters and details if you want."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
