{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3267e7b9",
   "metadata": {
    "id": "3267e7b9"
   },
   "source": [
    "# Course AI Homework 5\n",
    "In Homework 5, we will train our own 'CBOW' Word2Vec embedding from WikiText2 dataset. (small dataset)\n",
    "- Change Runtime option above to GPU if you could. (max 12 hours for one user)\n",
    "- Save and submit the outputs of this notebook and model and vocab file you trained.\n",
    "- Not allowed to have other python file or import pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c26099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torch as it is not installed.\n",
      "ERROR: Could not find a version that satisfies the requirement torch==2.3.0 (from versions: 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.9.1)\n",
      "ERROR: No matching distribution found for torch==2.3.0\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall torch -y\n",
    "! pip install torch==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fEUOIf5b55uz",
   "metadata": {
    "id": "fEUOIf5b55uz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torchtext==0.18.0 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1, 0.4.0, 0.5.0, 0.6.0)\n",
      "ERROR: No matching distribution found for torchtext==0.18.0\n"
     ]
    }
   ],
   "source": [
    "# YOU should run this command if you will train the model in COLAB environment\n",
    "! pip install datasets transformers torchtext==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83999a8f",
   "metadata": {
    "id": "83999a8f",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01margparse\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myaml\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import WikiText2 # WikiText103\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c48ed1",
   "metadata": {
    "id": "d1c48ed1"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_seed_numb = 0\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.manual_seed(torch_seed_numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020510ad",
   "metadata": {
    "id": "020510ad"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YYgqjYSdBvDW",
   "metadata": {
    "id": "YYgqjYSdBvDW"
   },
   "outputs": [],
   "source": [
    "# If you use Google Colab environment, mount you google drive here to save model and vocab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "root_dir = '/content/drive/MyDrive/course_ai_hw5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BNYyA8HX1Uz-",
   "metadata": {
    "id": "BNYyA8HX1Uz-"
   },
   "source": [
    "### Constant Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f991d",
   "metadata": {
    "id": "7f2f991d"
   },
   "outputs": [],
   "source": [
    "# You could change parameters if you want.\n",
    "\n",
    "train_batch_size =  96\n",
    "val_batch_size = 96\n",
    "shuffle =  True\n",
    "\n",
    "optimizer =  'Adam'\n",
    "learning_rate =  0.025\n",
    "epochs = 50\n",
    "\n",
    "result_dir = 'weights/'\n",
    "\n",
    "# Parameters about CBOW model architecture and Vocab.\n",
    "CBOW_N_WORDS = 4\n",
    "\n",
    "MIN_WORD_FREQUENCY = 50\n",
    "MAX_SEQUENCE_LENGTH = 256\n",
    "\n",
    "EMBED_DIMENSION = 300\n",
    "EMBED_MAX_NORM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3FgsAeWUDvzI",
   "metadata": {
    "id": "3FgsAeWUDvzI"
   },
   "outputs": [],
   "source": [
    "result_dir = os.path.join(root_dir, result_dir)\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xQacYYHCovzA",
   "metadata": {
    "id": "xQacYYHCovzA"
   },
   "source": [
    "## Prepare dataset and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rCx3DsJr5RxX",
   "metadata": {
    "id": "rCx3DsJr5RxX"
   },
   "outputs": [],
   "source": [
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "train_dataset = datasets[\"train\"]\n",
    "val_dataset = datasets['validation']\n",
    "test_dataset = datasets['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j499GQWrxOma",
   "metadata": {
    "id": "j499GQWrxOma"
   },
   "outputs": [],
   "source": [
    "# Let's print one example\n",
    "train_dataset['text'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulQoM_SixZBT",
   "metadata": {
    "id": "ulQoM_SixZBT"
   },
   "source": [
    "As you can see, we need to clean up the sentences, lowercase them, tokenize them, and change each word into an index (one-hot vector). Before going through the whole process, we need to create a vocab set using the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u63stoTf7oRn",
   "metadata": {
    "id": "u63stoTf7oRn"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "\n",
    "# TODO 1): make vocabulary\n",
    "# Hint) use function: build_vocab_from_iterator, use train_dataset set special tokens.. etc\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_dataset['text']),\n",
    "    min_freq=MIN_WORD_FREQUENCY,\n",
    "    specials=[\"<unk>\", \"<pad>\"]\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i8z22T68aT-m",
   "metadata": {
    "id": "i8z22T68aT-m"
   },
   "source": [
    "We need a collate function to make dataset into CBOW trainning format. The collate function should iterate over (sliding) batch data and make train/test dataset.And each component of data should be composed of CBOW_N_WORD words in each left and right side as input and target output as word in center.  \n",
    "Make the collate function return CBOW dataset in tensor type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CrDMIbXVHcbk",
   "metadata": {
    "id": "CrDMIbXVHcbk"
   },
   "outputs": [],
   "source": [
    "# Here is a lambda function to tokenize sentence and change words to vocab indexes.\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yl65bTOrEuSU",
   "metadata": {
    "id": "yl65bTOrEuSU"
   },
   "source": [
    "![cbow](https://user-images.githubusercontent.com/74028313/204695601-51d44a38-4bd3-4a69-8891-2854aa57c034.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745eedd1",
   "metadata": {
    "id": "745eedd1"
   },
   "outputs": [],
   "source": [
    "def collate(batch, text_pipeline):\n",
    "\n",
    "    batch_input, batch_output = [], []\n",
    "\n",
    "    # TODO 2): make collate function\n",
    "    for text in batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "        \n",
    "        if len(text_tokens_ids) < CBOW_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "        \n",
    "        for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):\n",
    "            context = (\n",
    "                text_tokens_ids[idx : idx + CBOW_N_WORDS] +\n",
    "                text_tokens_ids[idx + CBOW_N_WORDS + 1 : idx + CBOW_N_WORDS * 2 + 1]\n",
    "            )\n",
    "            target = text_tokens_ids[idx + CBOW_N_WORDS]\n",
    "            \n",
    "            batch_input.append(context)\n",
    "            batch_output.append(target)\n",
    "    \n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "\n",
    "    return batch_input, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c062f3",
   "metadata": {
    "id": "b8c062f3"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset['text'],\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=shuffle,\n",
    "    collate_fn=partial(collate, text_pipeline=text_pipeline),\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset['text'],\n",
    "    batch_size=val_batch_size,\n",
    "    shuffle=shuffle,\n",
    "    collate_fn=partial(collate, text_pipeline=text_pipeline),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95_n2reEdl8-",
   "metadata": {
    "id": "95_n2reEdl8-"
   },
   "source": [
    "## Make CBOW Model\n",
    "![image](https://user-images.githubusercontent.com/74028313/204701161-cd9df4bf-78b8-4b4d-b8b7-ed4a3b5c3922.png)\n",
    "\n",
    "CBOW Models' main concept is to predict center-target word using context words. As you see in above simple architecture, input 2XCBOW_N_WORDS length words are projected to Projection layer. In order to convert each word to embedding, it needs look-up table and we will use torch's Embedding function to convert it. After combining embeddings of context, it use shallow linear neural network to predict target word and compare result with center word's index using cross-entropy loss. Finally, the embedding layer (lookup table) of the trained model itself serves as an embedding representing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066a121",
   "metadata": {
    "id": "5066a121"
   },
   "outputs": [],
   "source": [
    "class CBOW_Model(nn.Module):\n",
    "    def __init__(self, vocab_size: int, EMBED_DIMENSION, EMBED_MAX_NORM):\n",
    "        super(CBOW_Model, self).__init__()\n",
    "        # TODO 3-1): make CBOW model using nn.Embedding and nn.Linear function\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "            max_norm=EMBED_MAX_NORM\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, _inputs):\n",
    "        # TODO 3-2): make forward function\n",
    "        x = self.embeddings(_inputs)\n",
    "        \n",
    "        x = x.mean(axis=1)\n",
    "        \n",
    "        _outputs = self.linear(x)\n",
    "\n",
    "        return _outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37edd6",
   "metadata": {
    "id": "3e37edd6"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Let's make _train_epoch and _validate_epoch functions to train the CBOW model.  \n",
    "- model.train() and model.eval() change torch mode in some parts (Dropout, BatchNorm..  etc) of the model to behave differently during inference time.\n",
    "- There is lr_scheduler option which changes learning rate according to epoch level. Try the option if you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gN4OgGATqry3",
   "metadata": {
    "id": "gN4OgGATqry3"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.get_stoi())\n",
    "\n",
    "model = CBOW_Model(vocab_size=vocab_size, EMBED_DIMENSION = EMBED_DIMENSION, EMBED_MAX_NORM = EMBED_MAX_NORM)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda8d29",
   "metadata": {
    "id": "0cda8d29"
   },
   "outputs": [],
   "source": [
    "class Train_CBOW:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        epochs,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        device,\n",
    "        model_dir,\n",
    "        lr_scheduler = None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.model_dir = model_dir\n",
    "\n",
    "        self.loss = {\"train\": [], \"val\": []}\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self._train_epoch()\n",
    "            self._validate_epoch()\n",
    "            print(\n",
    "                \"Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}\".format(\n",
    "                    epoch + 1,\n",
    "                    self.epochs,\n",
    "                    self.loss[\"train\"][-1],\n",
    "                    self.loss[\"val\"][-1],\n",
    "                )\n",
    "            )\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "\n",
    "    def _train_epoch(self):\n",
    "        self.model.train() # set model as train\n",
    "        loss_list = []\n",
    "        # TODO 4-1):\n",
    "        for batch_input, batch_output in self.train_dataloader:\n",
    "            batch_input = batch_input.to(self.device)\n",
    "            batch_output = batch_output.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            output = self.model(batch_input)\n",
    "            \n",
    "            loss = self.loss_function(output, batch_output)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "        # end of TODO\n",
    "        epoch_loss = np.mean(loss_list)\n",
    "        self.loss[\"train\"].append(epoch_loss)\n",
    "\n",
    "    def _validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        loss_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # TODO 4-2):\n",
    "            for batch_input, batch_output in self.val_dataloader:\n",
    "                batch_input = batch_input.to(self.device)\n",
    "                batch_output = batch_output.to(self.device)\n",
    "                \n",
    "                output = self.model(batch_input)\n",
    "                \n",
    "                loss = self.loss_function(output, batch_output)\n",
    "                \n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "            # end of TODO\n",
    "        epoch_loss = np.mean(loss_list)\n",
    "        self.loss[\"val\"].append(epoch_loss)\n",
    "\n",
    "\n",
    "    def save_model(self):\n",
    "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
    "        torch.save(self.model, model_path)\n",
    "\n",
    "    def save_loss(self):\n",
    "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
    "        with open(loss_path, \"w\") as fp:\n",
    "            json.dump(self.loss, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcx2Ms537jwR",
   "metadata": {
    "id": "fcx2Ms537jwR"
   },
   "outputs": [],
   "source": [
    "# Option: you could add and change lr_sceduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36edc874",
   "metadata": {
    "id": "36edc874"
   },
   "outputs": [],
   "source": [
    "trainer = Train_CBOW(\n",
    "    model=model,\n",
    "    epochs=epochs,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    loss_function=loss_function,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=None,\n",
    "    device=device,\n",
    "    model_dir=result_dir,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840866c3",
   "metadata": {
    "id": "840866c3"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.save_model()\n",
    "trainer.save_loss()\n",
    "\n",
    "vocab_path = os.path.join(result_dir, \"vocab.pt\")\n",
    "torch.save(vocab, vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373acbe",
   "metadata": {
    "id": "9373acbe"
   },
   "source": [
    "### Result\n",
    "Let's inference trained word embedding and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d19035",
   "metadata": {
    "id": "a6d19035"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fNszs2GvMbbz",
   "metadata": {
    "id": "fNszs2GvMbbz"
   },
   "outputs": [],
   "source": [
    "result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c1ad2",
   "metadata": {
    "id": "ba4c1ad2"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# reload saved model and vocab\n",
    "model = torch.load(os.path.join(result_dir,\"model.pt\"), map_location=device)\n",
    "vocab = torch.load(os.path.join(result_dir,\"vocab.pt\"))\n",
    "\n",
    "# embedding is model's first layer\n",
    "embeddings = list(model.parameters())[0]\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "\n",
    "# normalization\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
    "norms = np.reshape(norms, (len(norms), 1))\n",
    "embeddings_norm = embeddings / norms\n",
    "embeddings_norm.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arTAGVNbuxvP",
   "metadata": {
    "id": "arTAGVNbuxvP"
   },
   "source": [
    "### Make t-SNE graph of trained embedding and color numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2d8a7",
   "metadata": {
    "id": "e4e2d8a7"
   },
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(embeddings_norm)\n",
    "\n",
    "# TODO 5-1) : make 2-d t-SNE graph of all vocabs and color only for numeric values(others, just color black)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=30)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings_norm)\n",
    "\n",
    "words = vocab.get_itos()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "colors = []\n",
    "for word in words:\n",
    "    if word.isdigit():\n",
    "        digit_val = int(word)\n",
    "        colors.append(digit_val if digit_val < 10 else 9)\n",
    "    else:\n",
    "        colors.append(-1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "\n",
    "numeric_mask = np.array(colors) >= 0\n",
    "non_numeric_mask = ~numeric_mask\n",
    "\n",
    "plt.scatter(embeddings_tsne[non_numeric_mask, 0], \n",
    "           embeddings_tsne[non_numeric_mask, 1],\n",
    "           c='black', s=10, alpha=0.3, label='Non-numeric')\n",
    "\n",
    "if numeric_mask.sum() > 0:\n",
    "    scatter = plt.scatter(embeddings_tsne[numeric_mask, 0], \n",
    "                         embeddings_tsne[numeric_mask, 1],\n",
    "                         c=np.array(colors)[numeric_mask], \n",
    "                         cmap='tab10', s=50, alpha=0.8, \n",
    "                         edgecolors='black', linewidth=0.5)\n",
    "    plt.colorbar(scatter, label='Digit Value')\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings\\n(Numeric words colored)\", fontsize=16)\n",
    "plt.xlabel(\"t-SNE Component 1\", fontsize=12)\n",
    "plt.ylabel(\"t-SNE Component 2\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(result_dir, \"image.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image saved to {os.path.join(result_dir, 'image.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44537c",
   "metadata": {
    "id": "ce44537c"
   },
   "source": [
    "### Find top N similar words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa05e68",
   "metadata": {
    "id": "4fa05e68"
   },
   "outputs": [],
   "source": [
    "def find_top_similar(word: str, vocab, embeddings_norm, topN: int = 10):\n",
    "    # TODO 5-2) : make function returning top n similiar words and similarity scores\n",
    "    topN_dict = {}\n",
    "    \n",
    "    word_idx = vocab[word]\n",
    "    \n",
    "    word_embedding = embeddings_norm[word_idx]\n",
    "    \n",
    "    similarities = np.dot(embeddings_norm, word_embedding)\n",
    "    \n",
    "    top_indices = np.argsort(similarities)[::-1][:topN+1]\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        similar_word = vocab.lookup_token(idx)\n",
    "        if similar_word != word:\n",
    "            topN_dict[similar_word] = float(similarities[idx])\n",
    "            if len(topN_dict) == topN:\n",
    "                break\n",
    "\n",
    "    return topN_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74887441",
   "metadata": {
    "id": "74887441"
   },
   "outputs": [],
   "source": [
    "for word, sim in find_top_similar(\"english\", vocab, embeddings_norm).items():\n",
    "    print(\"{}: {:.3f}\".format(word, sim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h9WjlWvx0638",
   "metadata": {
    "id": "h9WjlWvx0638"
   },
   "source": [
    "### Result Report\n",
    "\n",
    "Save the colab result and submit it with your trained model file, vocab file, and t-SNE result image in the .zip format. Check one more time your submitted notebook file has result.\n",
    "\n",
    "You can change the CBOW model parameters Training parameters and details if you want."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
